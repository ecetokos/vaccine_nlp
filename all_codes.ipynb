{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58053aef",
   "metadata": {},
   "source": [
    "Verilerin Toplanması"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f52c66bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Snscrape kütüphanesi ile tweet çekme işlemi\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import csv\n",
    "\n",
    "# Arama parametrelerini \n",
    "keywords = [\"CoronaVac\", \"CoronaVak\", \"CoronaVacAsi\", \"CoronaVakAsi\",\n",
    "            \"Sinovac\", \"Sinovak\", \"SinovacAsi\", \"SinovakAsi\",\n",
    "            \"PfizerBioNTech\", \"PfizerBiontech\", \"PfizerBioNTechAsi\", \"PfizerBiontechAsi\",\n",
    "            \"Pfizer\", \"BioNTech\", \"BioNTek\",\n",
    "            \"SputnikV\", \"SputnikVAsi\",\n",
    "            \"TURKOVAC\", \"Turkovac\", \"TURKOVACAsi\", \"TurkovacAsi\"]   # Anahtar kelimeler\n",
    "start_date = \"2021-01-13\"  # Başlangıç tarihi (yyyy-mm-dd)\n",
    "end_date = \"2023-03-01\"  # Bitiş tarihi (yyyy-mm-dd)\n",
    "\n",
    "# Sorgu\n",
    "query = \" OR \".join(keywords) + f\" since:{start_date} until:{end_date} lang:tr\"\n",
    "\n",
    "# Tweetleri çekme\n",
    "tweets = []\n",
    "for tweet in sntwitter.TwitterSearchScraper(query).get_items():\n",
    "    tweets.append(tweet)\n",
    "\n",
    "# Verileri bir CSV dosyasına kaydetme\n",
    "filename = \"tweets.csv\"\n",
    "with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"ID\", \"User Name\", \"User Location\", \"Date\", \"Text\", \"Hashtags\", \"Source\"])\n",
    "    for tweet in tweets:\n",
    "        writer.writerow([tweet.id, tweet.user.username, tweet.user.location, tweet.date, tweet.content, tweet.hashtags, tweet.source])\n",
    "\n",
    "print(f\"{len(tweets)} tweet çekildi ve {filename} dosyasına kaydedildi.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f786d2a4",
   "metadata": {},
   "source": [
    "Veri Ön İşleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d001d39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85c1e441",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e83630",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('User Name', axis=1, inplace=True) #username sutunu kaldırıldı\n",
    "df.to_csv('df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "629038a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Source', axis=1, inplace=True) #Source sutunu kaldırıldı\n",
    "df.to_csv('df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60a34cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date']) \n",
    "df['Date'] = df['Date'].dt.strftime('%Y-%m-%d') #date formatı dönüştürüldü\n",
    "df.to_csv('df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776d4b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "SehirlerBolgeler = pd.read_csv(\"SehirlerBolgeler.csv\")\n",
    "SehirlerBolgeler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22039b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "iller_sozlugu = {\n",
    "    '(?i).*(adana).*': '1',\n",
    "    '(?i).*(adıyaman|adiyaman).*': '2',\n",
    "    '(?i).*(afyonkarahisar|afyon).*': '3',\n",
    "    '(?i).*(ağrı|agri).*': '4',\n",
    "    '(?i).*(amasya).*': '5',\n",
    "    '(?i).*(ankara).*': '6',\n",
    "    '(?i).*(antalya).*': '7',\n",
    "    '(?i).*(artvin).*': '8',\n",
    "    '(?i).*(aydın|aydin).*': '9',\n",
    "    '(?i).*(balıkesir|balikesir).*': '10',\n",
    "    '(?i).*(bilecik).*': '11',\n",
    "    '(?i).*(bingöl|bingol).*': '12',\n",
    "    '(?i).*(bitlis).*': '13',\n",
    "    '(?i).*(bolu).*': '14',\n",
    "    '(?i).*(burdur).*': '15',\n",
    "    '(?i).*(bursa).*': '16',\n",
    "    '(?i).*(çanakkale|canakkale).*': '17',\n",
    "    '(?i).*(çankırı|cankırı).*': '18',\n",
    "    '(?i).*(çorum|corum).*': '19',\n",
    "    '(?i).*(denizli).*': '20',\n",
    "    '(?i).*(diyarbakır|diyarbakir).*': '21',\n",
    "    '(?i).*(edirne).*': '22',\n",
    "    '(?i).*(elazığ|elazig).*': '23',\n",
    "    '(?i).*(erzincan).*': '24',\n",
    "    '(?i).*(erzurum).*': '25',\n",
    "    '(?i).*(eskişehir|eskisehir).*': '26',\n",
    "    '(?i).*(gaziantep|antep).*': '27',\n",
    "    '(?i).*(giresun).*': '28',\n",
    "    '(?i).*(gümüşhane|gümüshane).*': '29',\n",
    "    '(?i).*(hakkari).*': '30',\n",
    "    '(?i).*(hatay).*': '31',\n",
    "    '(?i).*(isparta|ısparta).*': '32',\n",
    "    '(?i).*(mersin).*': '33',\n",
    "    '(?i).*(istanbul|ıstanbul).*': '34',\n",
    "    '(?i).*(izmir).*': '35',\n",
    "    '(?i).*(kars).*': '36',\n",
    "    '(?i).*(kastamonu).*': '37',\n",
    "    '(?i).*(kayseri).*': '38',\n",
    "    '(?i).*(kırklareli|kirklareli).*': '39',\n",
    "    '(?i).*(kırşehir|kırsehir|kirsehir).*': '40',\n",
    "    '(?i).*(kocaeli).*': '41',\n",
    "    '(?i).*(konya).*': '42',\n",
    "    '(?i).*(kütahya|kutahya).*': '43',\n",
    "    '(?i).*(malatya).*': '44',\n",
    "    '(?i).*(manisa).*': '45',\n",
    "    '(?i).*(kahramanmaraş|kahramanmaras|maraş|maras).*': '46',\n",
    "    '(?i).*(mardin).*': '47',\n",
    "    '(?i).*(muğla|mugla).*': '48',\n",
    "    '(?i).*(muş|mus).*': '49',\n",
    "    '(?i).*(nevşehir|nevsehir).*': '50',\n",
    "    '(?i).*(niğde|nigde).*': '51',\n",
    "    '(?i).*(ordu).*': '52',\n",
    "    '(?i).*(rize).*': '53',\n",
    "    '(?i).*(sakarya).*': '54',\n",
    "    '(?i).*(samsun).*': '55',\n",
    "    '(?i).*(siirt).*': '56',\n",
    "    '(?i).*(sinop).*': '57',\n",
    "    '(?i).*(sivas).*': '58',\n",
    "    '(?i).*(tekirdağ|tekirdag).*': '59',\n",
    "    '(?i).*(tokat).*': '60',\n",
    "    '(?i).*(trabzon).*': '61',\n",
    "    '(?i).*(tunceli).*': '62',\n",
    "    '(?i).*(şanlıurfa|sanlıurfa|urfa).*': '63',\n",
    "    '(?i).*(uşak|usak).*': '64',\n",
    "    '(?i).*(van).*': '65',\n",
    "    '(?i).*(yozgat).*': '66',\n",
    "    '(?i).*(zonguldak).*': '67',\n",
    "    '(?i).*(aksaray).*': '68',\n",
    "    '(?i).*(bayburt).*': '69',\n",
    "    '(?i).*(karaman).*': '70',\n",
    "    '(?i).*(kırıkkale|kirikkale).*': '71',\n",
    "    '(?i).*(batman).*': '72',\n",
    "    '(?i).*(şırnak|sırnak).*': '73',\n",
    "    '(?i).*(bartın|bartin).*': '74',\n",
    "    '(?i).*(ardahan).*': '75',\n",
    "    '(?i).*(ığdır|ıgdir).*': '76',\n",
    "    '(?i).*(yalova).*': '77',\n",
    "    '(?i).*(karabük|karabuk).*': '78',\n",
    "    '(?i).*(kilis).*': '79',\n",
    "    '(?i).*(osmaniye).*': '80',\n",
    "    '(?i).*(düzce).*': '81'\n",
    "}\n",
    "#tc 81 il plaka noları"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b96801",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tc_il'] = 0\n",
    "df['tc_il'] = df['User Location'].replace(iller_sozlugu)\n",
    "df['tc_il'].fillna(-1, inplace=True)\n",
    "df['tc_il'] = df['tc_il'].replace(iller_sozlugu, regex=True)\n",
    "df['tc_il'] = np.where(df['tc_il'].str.isdigit(), df['tc_il'], 0)\n",
    "#81 il plaka, nan değeler -1, diğer 0 (-1,0,1:81)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1679296a",
   "metadata": {},
   "source": [
    "Aşı sınıf belirleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aed61c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "asi_sozluk = {\n",
    "    \"Sinovac\": [\"CoronaVac\", \"CoronaVak\", \"CoronaVacAsi\", \"CoronaVakAsi\",\n",
    "            \"Sinovac\", \"Sinovak\", \"SinovacAsi\", \"SinovakAsi\"],\n",
    "    \"Biontech\": [\"PfizerBioNTech\", \"PfizerBiontech\", \"PfizerBioNTechAsi\", \"PfizerBiontechAsi\",\n",
    "            \"Pfizer\", \"BioNTech\", \"BioNTek\"],\n",
    "    \"SputnikV\": [\"SputnikV\", \"SputnikVAsi\"],\n",
    "    \"Turkovac\": [\"TURKOVAC\", \"Turkovac\", \"TURKOVACAsi\", \"TurkovacAsi\"]\n",
    "}\n",
    "#4 aşı sınıflandırma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df04887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sınıf belirleme\n",
    "def belirle_sinif(text):\n",
    "    for sinif, kelimeler in asi_sozluk.items():\n",
    "        if any(kelime.lower() in text.lower() for kelime in kelimeler):\n",
    "            return sinif\n",
    "    return 'Diğer'\n",
    "\n",
    "# Yeni bir sınıf sütunu oluşturuldu ve sınıflar atandı\n",
    "df['Sınıf'] = df['Text'].apply(belirle_sinif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1864464a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35057310",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sınıf değerlerine göre text yazdırma\n",
    "sinovac_text = df[df['Sınıf'] == 'Sinovac']['Text']\n",
    "\n",
    "for text in sinovac_text:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cfbeab3",
   "metadata": {},
   "source": [
    "Veri Temizlik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67974b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "import demoji\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Noktalama işaretleri\n",
    "punctuation = string.punctuation.replace('@', '') + '’“”'\n",
    "\n",
    "# Stop kelimeleri\n",
    "stop_words = set(stopwords.words('turkish'))\n",
    "\n",
    "# Tweet tokenizer\n",
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "\n",
    "def clean_tweet(tweet):\n",
    "    # Küçük harfe dönüştürme\n",
    "    tweet = tweet.lower()\n",
    "    \n",
    "    # URL'leri temizleme\n",
    "    tweet = re.sub(r\"http\\S+|www\\S+|https\\S+\", '', tweet, flags=re.MULTILINE)\n",
    "    \n",
    "    # Kullanıcı adlarını temizleme\n",
    "    tweet = re.sub(r\"@\\w+\", '', tweet)\n",
    "    \n",
    "    # Hashtag'leri temizleme\n",
    "    tweet = re.sub(r\"#\\w+\", '', tweet)\n",
    "    \n",
    "    # Emoji'leri temizleme\n",
    "    tweet = demoji.replace(tweet, '')\n",
    "    \n",
    "    # Noktalama işaretlerini temizleme\n",
    "    tweet = ''.join([c for c in tweet if c not in punctuation])\n",
    "    \n",
    "    # Tokenizasyon\n",
    "    tokens = tokenizer.tokenize(tweet)\n",
    "    \n",
    "    # Stop kelimeleri temizleme\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Temizlenmiş tweet'i birleştirme\n",
    "    cleaned_tweet = ' '.join(tokens)\n",
    "    \n",
    "    return cleaned_tweet\n",
    "\n",
    "# Veri setindeki 'Tweet' sütununu temizleme\n",
    "df['New_Text'] = df['Text'].apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "687246b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)  # Tüm sütunları tam genişlikte görüntüleme\n",
    "\n",
    "# Text ve New_Text sütunlarını tüm satırlarıyla birlikte görüntüleme\n",
    "for index, row in df.iterrows():\n",
    "   # print(f\"Text: {row['Text']}\")\n",
    "    print(f\"New_Text: {row['New_Text']}\")\n",
    "    print(\"=\" * 50)  # Ayırıcı çizgi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7061d2",
   "metadata": {},
   "source": [
    "Veri Etiketleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3da5993",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = df[['ID', 'Text']].copy()\n",
    "label_df['Label'] = pd.Series(dtype='Int64')\n",
    "label_df.to_csv('label_df.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ab658ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeldf_etiketli = pd.read_csv(\"labeldf_etiketli.csv\")\n",
    "labeldf_etiketli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b3f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "value_counts = labeldf_etiketli['Label'].value_counts()\n",
    "print(value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6dbab97",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeldf_etiketli = labeldf_etiketli.head(783)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc7639ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeldf_etiketli['Label_name'] = labeldf_etiketli['Label'].replace({-1: 'negatif', 0: 'nötr', 1: 'pozitif'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c50958",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = Counter(labeldf_etiketli['Label_name']).keys()\n",
    "sum_ = Counter(labeldf_etiketli['Label_name']).values()\n",
    "df_sentiment = pd.DataFrame(zip(labels,sum_), columns = ['Kategori', 'Toplam'])\n",
    "df_sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8128bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "#etiketlerin görselleştirilmesi - çubuk grafiği\n",
    "df_sentiment.plot(x = 'Kategori' , y = 'Toplam',kind = 'bar', legend = False, grid = True, figsize = (15,5))\n",
    "plt.title('Kategori Sayılarının Görselleştirilmesi', fontsize = 20)\n",
    "plt.xlabel('Kategoriler', fontsize = 15)\n",
    "plt.ylabel('Toplam', fontsize = 15);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987f9c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#etiketlerin görselleştirilmesi - pasta grafiği\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.pie(df_sentiment.Toplam, labels =df_sentiment.Kategori, autopct = '%1.2f%%',  startangle = 90 )\n",
    "ax.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6570d613",
   "metadata": {},
   "source": [
    "Modelleme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5cda79",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df['New_Text'][:783].values.tolist()\n",
    "kategori = labeldf_etiketli['Label_name'].values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ebfc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#test ve train olarak ayırma \n",
    "x_train, x_test, y_train, y_test = train_test_split(text, kategori, test_size = 0.2, random_state = 42)\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.9, min_df=5)\n",
    "x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf = tfidf_vectorizer.transform(x_test)\n",
    "\n",
    "#tfidf \n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2), max_df=0.9, min_df = 5)\n",
    "\n",
    "x_train_tfidf = tfidf_vectorizer.fit_transform(x_train)\n",
    "x_test_tfidf = tfidf_vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab300c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression,SGDClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff7f948",
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regresyon\n",
    "model = OneVsRestClassifier(LogisticRegression(penalty = 'l2', C=1.0))\n",
    "model.fit(x_train_tfidf, y_train)\n",
    "\n",
    "print (\"Logistic Regression Accuracy={}\".format(accuracy_score(y_test, model.predict(x_test_tfidf))))\n",
    "logisticpred = accuracy_score(y_test, model.predict(x_test_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a63fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#stochastic gradient descent\n",
    "model2 = OneVsRestClassifier(SGDClassifier(loss = 'hinge', penalty = 'elasticnet', max_iter = 5))\n",
    "model2.fit(x_train_tfidf, y_train)\n",
    "print (\"SGD Accuracy={}\".format(accuracy_score(y_test, model2.predict(x_test_tfidf))))\n",
    "sgdpred = accuracy_score(y_test, model2.predict(x_test_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9916a6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#lineersvc\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "SVC_pipeline = Pipeline([\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])\n",
    "SVC_pipeline.fit(x_train_tfidf, y_train)\n",
    "\n",
    "prediction = SVC_pipeline.predict(x_test_tfidf)\n",
    "print('LineerSVC accuracy is {}'.format(accuracy_score(y_test, prediction)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829ef8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multinomial Naive Bayes \n",
    "nb_model = MultinomialNB()\n",
    "nb_model.fit(x_train_tfidf, y_train)\n",
    "\n",
    "nb_pred = nb_model.predict(x_test_tfidf)\n",
    "\n",
    "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
    "print(\"Naive Bayes Accuracy: {}\".format(nb_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff567d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest modeli \n",
    "rf_model = RandomForestClassifier(n_estimators=100)\n",
    "rf_model.fit(x_train_tfidf, y_train)\n",
    "\n",
    "rf_pred = rf_model.predict(x_test_tfidf)\n",
    "\n",
    "rf_accuracy = accuracy_score(y_test, rf_pred)\n",
    "print(\"Random Forest Accuracy: {}\".format(rf_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1078b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = labeldf_etiketli['Label_name']\n",
    "sinif = df['Sınıf'][0:783]\n",
    "merged_df = pd.concat([label_name, sinif], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca69194",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biontech\n",
    "vac_df = merged_df[merged_df['Sınıf'] == 'Biontech']  \n",
    "total_count = len(vac_df) \n",
    "\n",
    "positive_count = (vac_df['Label_name'] == 'pozitif').sum() \n",
    "negative_count = (vac_df['Label_name'] == 'negatif').sum() \n",
    "neutral_count = (vac_df['Label_name'] == 'nötr').sum() \n",
    "\n",
    "vac_df.groupby(\"Label_name\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d213a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinovac\n",
    "vac_df = merged_df[merged_df['Sınıf'] == 'Sinovac']  \n",
    "total_count = len(vac_df) \n",
    "\n",
    "positive_count = (vac_df['Label_name'] == 'pozitif').sum() \n",
    "negative_count = (vac_df['Label_name'] == 'negatif').sum() \n",
    "neutral_count = (vac_df['Label_name'] == 'nötr').sum() \n",
    "\n",
    "vac_df.groupby(\"Label_name\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616b9dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turkovac\n",
    "vac_df = merged_df[merged_df['Sınıf'] == 'Turkovac']  \n",
    "total_count = len(vac_df) \n",
    "\n",
    "positive_count = (vac_df['Label_name'] == 'pozitif').sum() \n",
    "negative_count = (vac_df['Label_name'] == 'negatif').sum() \n",
    "neutral_count = (vac_df['Label_name'] == 'nötr').sum() \n",
    "\n",
    "vac_df.groupby(\"Label_name\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7ef191",
   "metadata": {},
   "source": [
    "Analiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e758b31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"df.csv\")\n",
    "labeldf_etiketli = pd.read_csv(\"labeldf_etiketli.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf130d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_name = labeldf_etiketli['Label_name']\n",
    "sinif = df['Sınıf'][0:783]\n",
    "il = df['tc_il'][0:783]\n",
    "merged_df_il = pd.concat([label_name, sinif , il], axis=1)\n",
    "merged_df_il"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7478159c",
   "metadata": {},
   "outputs": [],
   "source": [
    "gruplu_df = merged_df_il.groupby('tc_il').size().reset_index(name='Gözlem Sayısı')\n",
    "sıralı_df = gruplu_df.sort_values(by='Gözlem Sayısı', ascending=False)\n",
    "print(sıralı_df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b759c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "il_max = df['tc_il'].value_counts().head(10)  #en çok tweet atılan şehirler\n",
    "il_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a35ecf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizer kullanarak n-gram analizi\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(2, 3), max_features=50)\n",
    "ngram_counts = ngram_vectorizer.fit_transform(df['New_Text'])\n",
    "\n",
    "# En çok geçen n-gram'ları ve frekanslarını al\n",
    "ngram_vocab = ngram_vectorizer.get_feature_names()\n",
    "ngram_freq = ngram_counts.sum(axis=0).A1\n",
    "ngram_dict = dict(zip(ngram_vocab, ngram_freq))\n",
    "\n",
    "# En çok geçen n-gram'ları görselleştirme\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(ngram_dict)), list(ngram_dict.values()), align='center')\n",
    "plt.xticks(range(len(ngram_dict)), list(ngram_dict.keys()), rotation=90)\n",
    "plt.xlabel('N-gram')\n",
    "plt.ylabel('Frekans')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# CountVectorizer kullanarak n-gram analizi\n",
    "ngram_vectorizer = CountVectorizer(ngram_range=(1, 1), max_features=50)\n",
    "ngram_counts = ngram_vectorizer.fit_transform(df['New_Text'])\n",
    "\n",
    "# En çok geçen n-gram'ları ve frekanslarını al\n",
    "ngram_vocab = ngram_vectorizer.get_feature_names()\n",
    "ngram_freq = ngram_counts.sum(axis=0).A1\n",
    "ngram_dict = dict(zip(ngram_vocab, ngram_freq))\n",
    "\n",
    "# En çok geçen n-gram'ları görselleştirme\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(len(ngram_dict)), list(ngram_dict.values()), align='center')\n",
    "plt.xticks(range(len(ngram_dict)), list(ngram_dict.keys()), rotation=90)\n",
    "plt.xlabel('N-gram')\n",
    "plt.ylabel('Frekans')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5015640c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doz_sozluk = {\n",
    "    \"Doz1\": [\"1.doz\", \"1. doz\", \"1 doz\", \"doz 1\"],\n",
    "    \"Doz2\": [\"2.doz\", \"2. doz\", \"2 doz\", \"doz 2\"],\n",
    "    \"Doz3\": [\"3.doz\", \"3. doz\", \"3 doz\", \"doz 3\"],\n",
    "    \"Doz4\": [\"4.doz\", \"4. doz\", \"4 doz\", \"doz 4\"],\n",
    "    \"Doz5\": [\"5.doz\", \"5. doz\", \"5 doz\", \"doz 5\"],\n",
    "}\n",
    "#doz sınıflandırma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3bb0513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# doz belirleme\n",
    "def belirle_sinif(text):\n",
    "    for sinif, kelimeler in doz_sozluk.items():\n",
    "        if any(kelime.lower() in text.lower() for kelime in kelimeler):\n",
    "            return sinif\n",
    "    return 'Diğer'\n",
    "\n",
    "df['Doz'] = df['New_Text'].apply(belirle_sinif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31884f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"Doz\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82f1196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doz oranları\n",
    "doz_values = [\"Doz1\", \"Doz2\", \"Doz3\", \"Doz4\", \"Doz5\"]\n",
    "filtered_df = df[df['Doz'].isin(doz_values)]\n",
    "doz_ratios = filtered_df['Doz'].value_counts(normalize=True) * 100\n",
    "other_df = df[~df['Doz'].isin(doz_values)]\n",
    "other_ratio = len(other_df) / len(df) * 100\n",
    "\n",
    "print(\"Doz Oranları:\")\n",
    "for doz, ratio in doz_ratios.iteritems():\n",
    "    print(f\"{doz}: {ratio:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f3b76e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "karsıtlık_sozluk = {\"var\": [\"ası olmadım\", \"aşı olmadım\", \"yaptırmadım\",\n",
    "                            \"aşı karsıtlıgı\", \"aşı karşıtlıgı\", \"aşıkarşıtlıgı\",\n",
    "                            \"aşı karsıtı\",\"aşı karşıtı\", \"asıkarsıtı\", \n",
    "                            \"aşı yapmama\",\"aşı yaptırma\",\"ası yaptırma\", \"ası yaptırma\"]}\n",
    "#ası karsıtlıgı sınıflandırma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8295422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# karsıtlık belirleme\n",
    "def belirle_sinif(text):\n",
    "    for sinif, kelimeler in karsıtlık_sozluk.items():\n",
    "        if any(kelime.lower() in text.lower() for kelime in kelimeler):\n",
    "            return sinif\n",
    "    return 'yok'\n",
    "\n",
    "df['Karşıtlık'] = df['Text'].apply(belirle_sinif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13e1dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Karşıtlık değerlerine göre gruplama\n",
    "karşıtlık_grup = df.groupby(\"Karşıtlık\").size()\n",
    "toplam_veri = len(df)\n",
    "karşıtlık_yüzde = karşıtlık_grup / toplam_veri * 100\n",
    "sonuç_df = pd.DataFrame({\"Sayısı\": karşıtlık_grup, \"Yüzde\": karşıtlık_yüzde})\n",
    "sonuç_df[\"Yüzde\"] = sonuç_df[\"Yüzde\"].map(\"{:.2f}%\".format)\n",
    "sonuç_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2974165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "yanetki_sozluk = {\n",
    "    \"ölüm\": [\"olum\",\"ölüm\",\"öldü\",\"öl\"],\n",
    "    \"miyokardit\": [\"kalp kası iltihabı\",\"kalp iltihaplanması\",\"miyokardit\",\"miyokadrit\"],\n",
    "    \"kalp\": [\"kalp\", \"kalb\", \"perikardit\",\"ritim\",\"damar\"],\n",
    "    \"pıhtı\" : [\"pıhtı\",\"pihti\"],\n",
    "    \"solunum\" : [\"nefes\",\"solunum\",\"astım\",\"astim\",\"öksürük\",\"oksuruk\"],\n",
    "    \"alerji\" : [\"alerji\",\"alerjı\"],\n",
    "    \"bağışıklık\" : [\"bağışıklık\",\"bagısıklık\",\"bağısıklık\",\"bagışıklık\",\"otoimmün\"],  \n",
    "    \"ateş\": [\"ateş\", \"ates\", \"titreme\"],\n",
    "    \"bulantı\": [\"bulantı\",\"bulanti\", \"mide bulantısı\", \"midem bul\", \"kus\",\"mide\"],\n",
    "    \"cilt\": [\"döküntü\",\"dokuntu\", \"cilt döküntüsü\", \"deri döküntüsü\",\"cilt\",\"deri\",\"egzama\",\"kurdesen\",\"kurdeşen\"\n",
    "             \"kızarma\",\"kızarıklık\",\"kizariklik\",\"kasıntı\",\"kaşıntı\",\"şişlik\",\"sislik\"],\n",
    "    \"ishal\": [\"bağırsak\", \"bagırsak\", \"ishal\"],\n",
    "    \"yorgunluk\": [\"yorgun\", \"bitkin\",\"bıtkın\",\"halsız\", \"halsiz\"],\n",
    "    \"ağrı\": [\"ağrı\",\"agrı\"],\n",
    "    \"lenf\": [\"lenf\"],\n",
    "    \"zona\": [\"zona\",\"varisella zoster\"],\n",
    "    \"felç\": [\"felç\",\"felc\"],\n",
    "}\n",
    "#yan etki sınıflandırma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d91a5b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yan etkilerin kontrol edildiği fonksiyon\n",
    "def kontrol_et(text):\n",
    "    yan_etkiler_sayac = {key: 0 for key in yanetki_sozluk}\n",
    "    for kelime in text.split():\n",
    "        for yan_etki, anahtar_kelimeler in yanetki_sozluk.items():\n",
    "            if kelime in anahtar_kelimeler:\n",
    "                yan_etkiler_sayac[yan_etki] += 1\n",
    "    return pd.Series(yan_etkiler_sayac)\n",
    "\n",
    "yan_etkiler_df = df['Text'].apply(kontrol_et)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e244349",
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = yan_etkiler_df.sum()\n",
    "counts = counts.sort_values( ascending=False)\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5cccd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "yanetki_sınıf = pd.concat([df['Sınıf'],yan_etkiler_df], axis=1)\n",
    "yanetki_sınıf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d5d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df = yanetki_sınıf.groupby('Sınıf').sum()\n",
    "grouped_df['Toplam'] = grouped_df.sum(axis=1)\n",
    "result_df = grouped_df.sort_values(by='Toplam', ascending=False)\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3c35fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.loc['Toplam'] = result_df.sum()\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6b199e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(2763/25796)*100 #tweetlerin yan etki içerme yüzdesi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ba746a",
   "metadata": {},
   "outputs": [],
   "source": [
    "(2363/2763)*100 #Biontech sınıfına dahil olanların yan etki içerme yüzdesi"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
